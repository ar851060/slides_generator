{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ar851060/slides_generator/blob/main/public_slides.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "G5Ex_rESutwL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd_RRqrHRNWB",
        "outputId": "967b7484-ac9d-442e-ace0-604d47e6fd62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-pptx\n",
            "  Downloading python_pptx-0.6.22-py3-none-any.whl (471 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/471.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/471.5 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m471.0/471.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.5/471.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-pptx) (4.9.3)\n",
            "Requirement already satisfied: Pillow<=9.5.0,>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from python-pptx) (9.4.0)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx)\n",
            "  Downloading XlsxWriter-3.1.2-py3-none-any.whl (153 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.0/153.0 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: XlsxWriter, python-pptx\n",
            "Successfully installed XlsxWriter-3.1.2 python-pptx-0.6.22\n",
            "Collecting openai\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.28.0\n"
          ]
        }
      ],
      "source": [
        "!pip install python-pptx\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai, json\n",
        "from pptx import Presentation\n",
        "from pptx.util import Inches, Pt, Cm\n",
        "from pptx.dml.color import RGBColor\n",
        "import time\n",
        "from google.colab import files, drive\n",
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "from pptx.enum.text import PP_ALIGN\n",
        "from PIL import Image, ImageEnhance\n",
        "import pickle\n",
        "from openai.embeddings_utils import cosine_similarity, get_embedding\n",
        "import time"
      ],
      "metadata": {
        "id": "WCiiamZCthKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# User control"
      ],
      "metadata": {
        "id": "ChONgxequxyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "money = 0\n",
        "openai.api_key = \"\"\n",
        "ppt_filename = \"\"\n",
        "topic = \"\"\n",
        "language = \"繁體中文\"\n",
        "number_pages = 8\n",
        "prs = Presentation(\"/content/new_template.pptx\")\n",
        "pic_resource = \"irasutoya\" # dalle, irasutoya, flaticon\n",
        "# if using irasutoya, you need to choose the method you want to pick up the pictures\n",
        "method = \"classification\" # classification, chat, random\n",
        "ppt_filename = \"\""
      ],
      "metadata": {
        "id": "LerVoLG7t4tF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# If you need to use irasutoya-classification method, then run this\n"
      ],
      "metadata": {
        "id": "jRXao4XNuahR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res = requests.get(f'https://www.irasutoya.com/p/mail.html')\n",
        "soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "li_list = soup.find_all(\"a\", dir = \"ltr\")\n",
        "label_list = []\n",
        "for li in li_list:\n",
        "    label_list.append(li.getText())\n",
        "\n",
        "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
        "label_embeddings = []\n",
        "\n",
        "for label in label_list:\n",
        "    embed = openai.Embedding.create(input = label, model=EMBEDDING_MODEL)\n",
        "    label_embeddings.append(embed['data'][0]['embedding'])\n",
        "    money += embed[\"usage\"][\"total_tokens\"] / 1000 * 0.0001\n",
        "    time.sleep(3)\n",
        "\n",
        "print(money)"
      ],
      "metadata": {
        "id": "0MIo7OOItu6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# picture generator"
      ],
      "metadata": {
        "id": "3QRyg0JzkAzK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqk-Wd3nmsTN"
      },
      "outputs": [],
      "source": [
        "def irasudoya_pics(slide, money, get_keywords_methods:str=\"classification\", number_keywords:int=3, temperature:float=0):\n",
        "\n",
        "    res = requests.get(f'https://www.irasutoya.com/p/mail.html')\n",
        "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "    li_list = soup.find_all(\"a\", dir = \"ltr\")\n",
        "    label_list = []\n",
        "    for li in li_list:\n",
        "        label_list.append(li.getText())\n",
        "\n",
        "    def get_keywords(text, money, number_keywords):\n",
        "        drive.mount('/content/drive')\n",
        "\n",
        "        EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
        "        embed = openai.Embedding.create(input = text, model=EMBEDDING_MODEL)\n",
        "        text_embeddings = embed['data'][0]['embedding']\n",
        "        money += embed[\"usage\"][\"total_tokens\"] / 1000 * 0.0001\n",
        "\n",
        "        with open(\"/content/drive/MyDrive/label_embeddings\", \"rb\") as fp:\n",
        "            label_embeddings = pickle.load(fp)\n",
        "\n",
        "        def label_score(text_embeddings, label_embeddings):\n",
        "            score = []\n",
        "            for label in label_embeddings:\n",
        "                cos = cosine_similarity(text_embeddings, label)\n",
        "                score.append(cos)\n",
        "            return score\n",
        "\n",
        "        score = label_score(text_embeddings,label_embeddings)\n",
        "        score = score / np.sum(score)\n",
        "        key_label = np.random.choice(label_list, number_keywords, replace=False, p=score)\n",
        "\n",
        "        return key_label, money\n",
        "\n",
        "    def get_keywords_chat(text, money, number_keywords, temperature):\n",
        "\n",
        "        find_pic_json = \"\"\"{\n",
        "        \"input_text\": \"[[QUERY]]\",\n",
        "        \"output_format\": \"json\",\n",
        "        \"json_structure\": {\n",
        "            \"keywords\":['{{keywords}}','{{keywords}}','{{keywords}}']\n",
        "        }\n",
        "        }\"\"\"\n",
        "\n",
        "        question_text = f\"\"\"\n",
        "        According the content of this page, pick up {number_keywords} the most relevant Japanese keywords, and sort from the most relevant to the least relevant. Format as json.\n",
        "        \"\"\"\n",
        "\n",
        "        question = question_text+\"\"\"\n",
        "        the structure of json should be 'keywords':['{{keywords}}','{{keywords}}','{{keywords}}']。The list of Japanese keywords is following:###\n",
        "        \"\"\" + \",\".join(label_list)\n",
        "\n",
        "        prompt = find_pic_json.replace(\"[[QUERY]]\",question)\n",
        "\n",
        "        completion = openai.ChatCompletion.create(model = \"gpt-3.5-turbo\", temperature = temperature, messages =[\n",
        "            {\"role\":\"user\",\"content\":\"Please create the content of next page slide\"},\n",
        "            {\"role\":\"assistant\", \"content\":f\"Header:{slide['header']}, Content:{slide['content']}\"},\n",
        "            {\"role\":\"user\",\"content\":prompt}\n",
        "            ])\n",
        "\n",
        "        try:\n",
        "            key_label = json.loads(completion.choices[0].message.content)[\"keywords\"]\n",
        "        except:\n",
        "            key_label = json.loads(completion.choices[0].message.content)[\"json_structure\"][\"keywords\"]\n",
        "        money = money + completion.usage.prompt_tokens * 0.0015 / 1000 + completion.usage.completion_tokens * 0.002 / 1000\n",
        "\n",
        "        return key_label, money\n",
        "\n",
        "\n",
        "    text = slide['content']\n",
        "    if get_keywords_methods == \"classification\":\n",
        "        keywords_pic, money = get_keywords(text, money, number_keywords)\n",
        "    elif get_keywords_methods == \"chat\":\n",
        "        keywords_pic, money = get_keywords_chat(text, money, number_keywords, temperature)\n",
        "    elif get_keywords_methods == \"random\":\n",
        "        keywords_pic = np.random.choice(label_list, size=number_keywords, replace=False)\n",
        "    else:\n",
        "        assert False, f\"there are only two methods to get pictures: 'classification', 'compress', and 'chat'. {get_keywords_methods} is not including.\"\n",
        "\n",
        "    print(keywords_pic)\n",
        "    print(f\"Spent:{money}\")\n",
        "    pic_name = None\n",
        "    while pic_name == None:\n",
        "        for i in range(3,0,-1):\n",
        "            keywords_string = \"+\".join(keywords_pic[:i])\n",
        "            link_of_search = f'https://www.irasutoya.com/search/label/{keywords_string}'\n",
        "            res = requests.get(link_of_search)\n",
        "            soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "            find_pics = soup.find_all('div', id='post', class_='box')\n",
        "            if find_pics:\n",
        "                first_search = find_pics[np.random.randint(len(find_pics))].find(\"a\")[\"href\"]\n",
        "                res = requests.get(first_search)\n",
        "                soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "                div = soup.find_all('div', class_='separator')\n",
        "                div_len = len(div)\n",
        "                for i in range(div_len):\n",
        "                    if \"img\" in str(div[i]):\n",
        "                        pic_link = div[i].find(\"img\")[\"src\"]\n",
        "                        if \"http\" in pic_link:\n",
        "                            img = requests.get(pic_link)\n",
        "                        else:\n",
        "                            img = requests.get(\"https:\"+pic_link)\n",
        "\n",
        "                        pic_name = re.findall(\"s[0-9]+/(.*g)$\",pic_link)[0]\n",
        "                        with open(f\"{pic_name}\", \"wb\") as file:\n",
        "                            file.write(img.content)\n",
        "                        time.sleep(2)\n",
        "\n",
        "                        return pic_name, money\n",
        "        keywords_pic = np.random.choice(label_list, size=number_keywords, replace=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dM30x2sEU6hd"
      },
      "outputs": [],
      "source": [
        "def dalle_pics(slide, money, number_keywords:int=3, temperature:float = 0.9, size_picture:int=256, prompt:str=None):\n",
        "\n",
        "    assert str(size_picture) in [\"256\",\"512\",\"1024\"], f\"Dall-e only support the size of picture in 256, 512, and 1024. {size_picture} is not supported.\"\n",
        "\n",
        "    text = f\"According to this page, help me to extract {number_keywords} english keywords which are most relevant, and sort from the most relevant to the least. Format as json.\"\n",
        "\n",
        "    find_pic_json = \"\"\"{\n",
        "        \"input_text\": \"\"\" + text + \"\"\"\n",
        "        The json construct should be 'keywords':['{{keywords}}','{{keywords}}','{{keywords}}']”,\n",
        "        \"output_format\": \"json\",\n",
        "        \"json_structure\": {\n",
        "            \"keywords\":['{{keywords}}','{{keywords}}','{{keywords}}']\n",
        "        }\n",
        "        }\"\"\"\n",
        "\n",
        "    completion = openai.ChatCompletion.create(model = \"gpt-3.5-turbo\", temperature = temperature, messages =[\n",
        "        {\"role\":\"user\",\"content\":\"Please create the content of next page slide\"},\n",
        "        {\"role\":\"assistant\", \"content\":f\"Header:{slide['header']}, Content:{slide['content']}\"},\n",
        "        {\"role\":\"user\",\"content\":find_pic_json}\n",
        "        ])\n",
        "    try:\n",
        "        keywords_pic = json.loads(completion.choices[0].message.content)[\"keywords\"]\n",
        "    except:\n",
        "        keywords_pic = json.loads(completion.choices[0].message.content)[\"json_structure\"][\"keywords\"]\n",
        "    money = money + completion.usage.prompt_tokens * 0.0015 / 1000 + completion.usage.completion_tokens * 0.002 / 1000\n",
        "    print(f\"Spent:{money}\")\n",
        "\n",
        "    keys = \",\".join(keywords_pic)\n",
        "    if prompt:\n",
        "        prompt = prompt.format(keywords = keys)\n",
        "        response = openai.Image.create(\n",
        "        prompt=prompt,\n",
        "        n=1,\n",
        "        size=f\"{size_picture}x{size_picture}\"\n",
        "        )\n",
        "    else:\n",
        "        response = openai.Image.create(\n",
        "        prompt=f\"{keys};svg colored icon with blank background\",\n",
        "        n=1,\n",
        "        size=f\"{size_picture}x{size_picture}\"\n",
        "        )\n",
        "\n",
        "    time.sleep(2)\n",
        "    image_url = response['data'][0]['url']\n",
        "    money += 0.016\n",
        "    print(f\"Spent:{money}\\n\")\n",
        "    img = requests.get(image_url)\n",
        "    pic_name = \"\".join(keywords_pic)+\".png\"\n",
        "    with open(f\"{pic_name}\", \"wb\") as file:\n",
        "        file.write(img.content)\n",
        "    return pic_name, money"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7n0Qj-qe53k"
      },
      "outputs": [],
      "source": [
        "def flaticon_pics(slide, money, number_keywords:int=3, temperature:float=0.9, size_picture:int=256):\n",
        "    def pics_flaticon(keywords, size_picture):\n",
        "        for key in keywords:\n",
        "            link_of_search = f'https://www.flaticon.com/search?word={key}'\n",
        "            res = requests.get(link_of_search, headers={\"user-agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36\"})\n",
        "            soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "            find_pics = soup.find_all('a', class_='view link-icon-detail')\n",
        "            if find_pics and size_picture > 128:\n",
        "                return find_pics[np.random.randint(len(find_pics))].find(\"img\")[\"data-src\"].replace(\"128\",str(size_picture))\n",
        "        return None\n",
        "\n",
        "    assert str(size_picture) in [\"128\",\"256\",\"512\"], f\"The picture in Flaticon only support the size of 128, 256, and 512, {size_picture} is not supported.\"\n",
        "\n",
        "    text = f\"\"\"\n",
        "    extract {number_keywords} the most relevant English keywords, and sort from the most relevant to the least relevant. Format as json.\n",
        "    \"\"\"\n",
        "\n",
        "    find_pic_json = \"\"\"{\n",
        "        \"input_text\": \"According to the content of this page,\"\"\" + text +\"\"\"\n",
        "                The structure of json should be 'keywords':['{{keywords}}','{{keywords}}','{{keywords}}']\",\n",
        "        \"output_format\": \"json\",\n",
        "        \"json_structure\": {\n",
        "            \"keywords\":['{{keywords}}','{{keywords}}','{{keywords}}']\n",
        "        }\n",
        "        }\"\"\"\n",
        "\n",
        "    while True:\n",
        "        completion = openai.ChatCompletion.create(model = \"gpt-3.5-turbo\", temperature = temperature, messages =[\n",
        "            {\"role\":\"user\",\"content\":\"Please create the content of next page slide\"},\n",
        "            {\"role\":\"assistant\", \"content\":f\"Header:{slide['header']}, Content:{slide['content']}\"},\n",
        "            {\"role\":\"user\",\"content\":find_pic_json}\n",
        "            ])\n",
        "        print(completion.choices[0].message.content)\n",
        "        try:\n",
        "            keywords_pic = json.loads(completion.choices[0].message.content)[\"keywords\"]\n",
        "        except:\n",
        "            keywords_pic = json.loads(completion.choices[0].message.content)[\"json_structure\"][\"keywords\"]\n",
        "        money = money + completion.usage.prompt_tokens * 0.0015 / 1000 + completion.usage.completion_tokens * 0.002 / 1000\n",
        "        print(f\"Spent:{money}\")\n",
        "        pic_link = pics_flaticon(keywords_pic)\n",
        "        if temp < 2:\n",
        "            temp += 0.1\n",
        "        if pic_link is not None:\n",
        "            break\n",
        "    img = requests.get(pic_link)\n",
        "    pic_name = re.findall(f\"{size_picture}/[0-9]+/(.*g)$\",pic_link)[0]\n",
        "    with open(f\"{pic_name}\", \"wb\") as file:\n",
        "        file.write(img.content)\n",
        "\n",
        "    return pic_name, money"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSK3eKPmp8hk"
      },
      "source": [
        "# The full process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W242DE9wtiIP",
        "outputId": "d2f6356a-1291-4a65-987b-e75ff508dcf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "開始\n",
            "已經花了:0.002842\n",
            "標題：日本四季節慶介紹\n",
            "\n",
            "資訊圖卡 1:\n",
            "標題：春天的櫻花季\n",
            "文字：日本的櫻花季是一年中最受歡迎的季節之一。從三月至四月，櫻花盛開，人們會參加賞花活動、野餐和櫻花祭典，以慶祝這個美麗的季節。\n",
            "\n",
            "資訊圖卡 2:\n",
            "標題：夏天的盆踊祭\n",
            "文字：夏天，日本人會參加盆踊祭，這是一種夏季盛會。人們穿著傳統服裝，在戶外跳舞、玩遊戲和品嚐傳統美食。這是一個歡樂的活動，讓人們在炎熱的夏天放鬆身心。\n",
            "\n",
            "資訊圖卡 3:\n",
            "標題：秋天的紅葉季\n",
            "文字：秋天，日本的山區會變得滿是美麗的紅葉。人們會去登山、賞紅葉和品嚐秋季限定的美食。紅葉季是一個獨特而迷人的季節，吸引著遊客和本地人。\n",
            "\n",
            "資訊圖卡 4:\n",
            "標題：冬天的雪祭\n",
            "文字：冬天，北海道的札幌舉辦了著名的雪祭，展示各種雕刻和巨大的冰雕。遊客可以欣賞這些冰雕，參加冰壺比賽和享受傳統的溫泉浴。雪祭是一個充滿冬季氛圍和樂趣的活動。\n",
            "\n",
            "資訊圖卡 5:\n",
            "標題：清明祭\n",
            "文字：日本的清明祭是一個重要的傳統活動，用來追悼和悼念祖先。人們會去祖墳、清理墓地和供奉食物和花朵。這是一個平靜而有意義的時刻，讓人們思考並與過去的連結。\n",
            "\n",
            "資訊圖卡 6:\n",
            "標題：七五三節\n",
            "文字：七五三節是一個慶祝孩子們成長的節日。在這個節日，三歲、五歲和七歲的孩子會穿上傳統服裝，參拜神社，祈求健康和幸福。這是一個充滿家庭和愛的美好時刻。\n",
            "\n",
            "資訊圖卡 7:\n",
            "標題：新年的初詣\n",
            "文字：新年，日本人會參加初詣儀式，去神社祈求新的一年的好運和健康。人們會買小福袋、吃傳統食物和觀賞節目，以迎接新的開始。\n",
            "\n",
            "資訊圖卡 8:\n",
            "標題：祇園祭\n",
            "文字：每年七月，京都舉辦著名的祇園祭。這個祭典包括各種活動，如巨大的神轎遊行、表演和傳統的體育比賽。祇園祭是京都最重要的節日之一，吸引著來自世界各地的遊客。\n",
            "\n",
            "希望這些資訊圖卡能為你帶來對日本四季節慶的更多了解！如果你還有其他問題或需要更多資訊，歡迎提問！\n",
            "大綱完成\n"
          ]
        }
      ],
      "source": [
        "print(\"Start\")\n",
        "\n",
        "completion = openai.ChatCompletion.create(model = \"gpt-3.5-turbo\",temperature = 0.9, messages = [\n",
        "    {\"role\":\"system\",\"content\":f\"\"\"\n",
        "    Write for a curious audience of college students in a friendly, conversational tone. You are an expert on this topic and want to share your knowledge with the world.\n",
        "    Please follow these instructions in all your responses:\n",
        "    1. Only use {language} language;\n",
        "    2. Use {language} text as much as possible;\n",
        "    3. Translate any other languages ​​into {language} where possible.\n",
        "    4. The outlines are Mutually Exclusive Collectively Exhaustive.\n",
        "    \"\"\"},\n",
        "    {\"role\":\"user\",\"content\":f\"\"\"Please write a series of {language} {number_pages}-paged infographs，and the topic is '{topic}'. These infographs will be put on instrgram.\n",
        "    Every infograph including 60 to 80 vocabulary, and give each infograph a attractive title.\n",
        "    DO NOT generate covers, tags, the desciption of pictures. Do NOT need reference.\"\"\"}\n",
        "    ])\n",
        "response = completion.choices[0].message.content\n",
        "money = money + completion.usage.prompt_tokens * 0.0015 / 1000 + completion.usage.completion_tokens * 0.002 / 1000\n",
        "print(f\"Spent:{money}\\n\")\n",
        "print(response)\n",
        "print(\"Outline Finished\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-aTb5PS7dDe",
        "outputId": "d58749eb-b229-4dbe-831f-0e345daaf28b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "已經花了:0.007221\n",
            "內容完成\n"
          ]
        }
      ],
      "source": [
        "query_json = \"\"\"{\n",
        "    \"input_text\": \"[[QUERY]]\",\n",
        "    \"output_format\": \"json\",\n",
        "    \"json_structure\": {\n",
        "        \"title\":\"{{title}}\",\n",
        "        \"slides\":\"{{slides}}\"\n",
        "       }\n",
        "    }\"\"\"\n",
        "\n",
        "question = f\"\"\"\n",
        "According to the outlines of infographs above, reformat a series of {language} {number_pages}-paged infographs as json format, and come up a attractive title for this series of inforgraphs.\n",
        "These will be put on instragram.\n",
        "The structure of json should be “slides”:“{{slides}}”,the format of title is 'title':{{title}}，and each infographs should have a {{header}},{{content}}.\n",
        "header is the attractive title of that infographs, and it should be less than 8 vocabulary.\n",
        "The content should be showed as paragraphs.\n",
        "\"\"\"\n",
        "\n",
        "prompt = query_json.replace(\"[[QUERY]]\",question)\n",
        "\n",
        "completion = openai.ChatCompletion.create(model = \"gpt-3.5-turbo\",temperature = 0.9, messages = [\n",
        "    {\"role\":\"system\",\"content\":f\"Write for a curious audience of college students in a friendly, conversational tone. You are an expert on this topic and want to share your knowledge with the world.\"},\n",
        "    {\"role\":\"assistant\",\"content\":response},\n",
        "    {\"role\":\"user\",\"content\":prompt}\n",
        "    ])\n",
        "response = completion.choices[0].message.content\n",
        "money = money + completion.usage.prompt_tokens * 0.0015 / 1000 + completion.usage.completion_tokens * 0.002 / 1000\n",
        "print(f\"Spent:{money}\")\n",
        "print(\"Content Finished\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwqPwIj9s10E"
      },
      "outputs": [],
      "source": [
        "r = json.loads(response)\n",
        "\n",
        "prs.slides[0].shapes.title.text = r[\"title\"]\n",
        "prs.slides[0].shapes.title.text_frame.paragraphs[0].font.color.rgb = RGBColor(64,64,64)\n",
        "\n",
        "\n",
        "slide_data = r[\"slides\"]\n",
        "\n",
        "for slide in slide_data:\n",
        "\n",
        "\n",
        "    if pic_resource == \"flaticon\":\n",
        "        pic_name, money = flaticon_pics(slide, money)\n",
        "    elif pic_resource == \"dalle\":\n",
        "        pic_name, money = dalle_pics(slide, money)\n",
        "    elif pic_resource == \"irasutoya\":\n",
        "        pic_name, money = irasudoya_pics(slide, money, get_keywords_methods = method)\n",
        "    else:\n",
        "        assert False, f\"We can only support three kinds of picture sources: flaticon, dall-e, irasudoya. {pic_resource} is not one of them.\"\n",
        "\n",
        "    time.sleep(2)\n",
        "\n",
        "    img = Image.open(pic_name)\n",
        "    img = np.array(img)\n",
        "    if len(img.shape)>2:\n",
        "        try:\n",
        "            img[:, :, 3] = img[:, :, 3] * 0.2\n",
        "        except:\n",
        "            layer = np.ones((img.shape[0],img.shape[1],1))*0.2\n",
        "            result = np.append(img, layer, axis=2)\n",
        "        img = Image.fromarray(img)\n",
        "        img.save(pic_name)\n",
        "\n",
        "\n",
        "\n",
        "    slide_layout = prs.slide_layouts[1]\n",
        "    new_slide = prs.slides.add_slide(slide_layout)\n",
        "    pic = new_slide.shapes.add_picture(f\"/content/{pic_name}\", Cm(7.73), Cm(10.91), width = Cm(22.64), height = Cm(22.95))\n",
        "    # pic = new_slide.shapes.add_picture(f\"/content/{pic_name}\", Cm(3.57), Cm(3.18), width = Cm(7.54), height = Cm(7.54))\n",
        "\n",
        "\n",
        "    if slide['header']:\n",
        "        title = new_slide.shapes.title\n",
        "        title.text = slide['header']\n",
        "        title.text_frame.paragraphs[0].font.size = Pt(100)\n",
        "        title.text_frame.paragraphs[0].font.underline = True\n",
        "\n",
        "\n",
        "    if slide['content']:\n",
        "        shapes = new_slide.shapes\n",
        "        body_shape = shapes.placeholders[1]\n",
        "        tf = body_shape.text_frame\n",
        "        tf.clear()\n",
        "        tf.text = slide['content'].replace([\"。\",\".\"],\"\\n\")\n",
        "        paragraph_num = len(tf.paragraphs)\n",
        "        for i in range(paragraph_num):\n",
        "            p = tf.paragraphs[i]\n",
        "            p.level = 0\n",
        "            p.font.size = Pt(40)\n",
        "            p.font.bold = True\n",
        "            p.line_spacing = Pt(60)\n",
        "            p.font.color.rgb = RGBColor(0,0,0)\n",
        "            p.alignment = PP_ALIGN.CENTER\n",
        "\n",
        "    new_slide.shapes._spTree.insert(2, pic._element)\n",
        "\n",
        "\n",
        "print(f\"Spent:{money}\")\n",
        "prs.save(f\"{ppt_filename}.pptx\")\n",
        "files.download(f\"/content/{ppt_filename}.pptx\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "G5Ex_rESutwL",
        "jRXao4XNuahR",
        "3QRyg0JzkAzK"
      ],
      "mount_file_id": "1fMjcoCi-CBM7vHnzc4FeIbwotacyzxtU",
      "authorship_tag": "ABX9TyP5KPsP+5Nc9Bv96muQ8kQv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}